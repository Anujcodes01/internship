{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05ee434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95a42b9",
   "metadata": {},
   "source": [
    "## Importing Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5de1831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e6c6d9",
   "metadata": {},
   "source": [
    "## 1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0255407",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_header_tags(url):\n",
    "    \n",
    "    # send request to the server\n",
    "    response = requests.get(url)\n",
    "    # parse the html content with beautiful soup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    # finding the data\n",
    "    header_tags = []\n",
    "    for i in range(1, 7):\n",
    "        # extract the headers from the website\n",
    "        headers = soup.find_all(\"h{}\".format(i))\n",
    "        header_tags.extend(headers)\n",
    "    # convert the data into text\n",
    "    header_texts = [header.text for header in header_tags]\n",
    "    # create dataframe\n",
    "    df = pd.DataFrame(header_texts, columns=[\"Header\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a4b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Header\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "df = get_header_tags(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b59869",
   "metadata": {},
   "source": [
    "## 2) Write a python program to display IMDB’s Top rated 50 movies’ data (i.e. name, rating, year of release) and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd123b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_50_movies(url):\n",
    "    \n",
    "    ## Send HTTP request to IMDb Top 250 page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # find all top movies data\n",
    "    top_movies = soup.find_all('td',class_ = \"titleColumn\")\n",
    "    \n",
    "    # extract name rating year for the each movie\n",
    "    \n",
    "    movie_names = []\n",
    "    movie_ratings = []\n",
    "    movie_years = []\n",
    "    \n",
    "    for movies in top_movies[:50]:\n",
    "        \n",
    "        \n",
    "        #extract name\n",
    "        name = movies.a.text\n",
    "        movie_names.append(name)\n",
    "        \n",
    "        #extract rating of the movies\n",
    "         \n",
    "        rating = movies.parent.find(\"td\", class_=\"ratingColumn imdbRating\").strong.text\n",
    "        movie_ratings.append(rating)\n",
    "        \n",
    "        \n",
    "        # extract year of release\n",
    "        year = movies.span.text.strip(\"()\")\n",
    "        movie_years.append(year)\n",
    "    \n",
    "    \n",
    "    # Create DataFrame \n",
    "    df = pd.DataFrame({\n",
    "        'Name':movie_names,\n",
    "        'Rating':movie_ratings,\n",
    "        'Year':movie_years\n",
    "        \n",
    "        })\n",
    "    \n",
    "    return df\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1654bdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Name Rating  Year\n",
      "0                            The Shawshank Redemption    9.2  1994\n",
      "1                                       The Godfather    9.2  1972\n",
      "2                                     The Dark Knight    9.0  2008\n",
      "3                               The Godfather Part II    9.0  1974\n",
      "4                                        12 Angry Men    9.0  1957\n",
      "5                                    Schindler's List    8.9  1993\n",
      "6       The Lord of the Rings: The Return of the King    8.9  2003\n",
      "7                                        Pulp Fiction    8.8  1994\n",
      "8   The Lord of the Rings: The Fellowship of the Ring    8.8  2001\n",
      "9                     Il buono, il brutto, il cattivo    8.8  1966\n",
      "10                                       Forrest Gump    8.8  1994\n",
      "11                                         Fight Club    8.7  1999\n",
      "12              The Lord of the Rings: The Two Towers    8.7  2002\n",
      "13                                          Inception    8.7  2010\n",
      "14                            The Empire Strikes Back    8.7  1980\n",
      "15                                         The Matrix    8.7  1999\n",
      "16                                         GoodFellas    8.7  1990\n",
      "17                    One Flew Over the Cuckoo's Nest    8.6  1975\n",
      "18                                              Se7en    8.6  1995\n",
      "19                               Shichinin no samurai    8.6  1954\n",
      "20                              It's a Wonderful Life    8.6  1946\n",
      "21                           The Silence of the Lambs    8.6  1991\n",
      "22                                Saving Private Ryan    8.6  1998\n",
      "23                                     Cidade de Deus    8.6  2002\n",
      "24                                       Interstellar    8.6  2014\n",
      "25                                    La vita è bella    8.6  1997\n",
      "26                                     The Green Mile    8.6  1999\n",
      "27                                          Star Wars    8.5  1977\n",
      "28                         Terminator 2: Judgment Day    8.5  1991\n",
      "29                                 Back to the Future    8.5  1985\n",
      "30                      Sen to Chihiro no kamikakushi    8.5  2001\n",
      "31                                        The Pianist    8.5  2002\n",
      "32                                             Psycho    8.5  1960\n",
      "33                                       Gisaengchung    8.5  2019\n",
      "34                                               Léon    8.5  1994\n",
      "35                                      The Lion King    8.5  1994\n",
      "36                                          Gladiator    8.5  2000\n",
      "37                                 American History X    8.5  1998\n",
      "38                                       The Departed    8.5  2006\n",
      "39                                       The Prestige    8.5  2006\n",
      "40                                           Whiplash    8.5  2014\n",
      "41                                 The Usual Suspects    8.5  1995\n",
      "42                                         Casablanca    8.5  1942\n",
      "43                                     Hotaru no haka    8.5  1988\n",
      "44                                            Seppuku    8.5  1962\n",
      "45                                   The Intouchables    8.5  2011\n",
      "46                                       Modern Times    8.4  1936\n",
      "47                       Once Upon a Time in the West    8.4  1968\n",
      "48                              Nuovo Cinema Paradiso    8.4  1988\n",
      "49                                        Rear Window    8.4  1954\n"
     ]
    }
   ],
   "source": [
    "top_movies = get_top_50_movies('https://www.imdb.com/chart/top/')\n",
    "print(top_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879ec5a",
   "metadata": {},
   "source": [
    "## 3) Write a python program to display IMDB’s Top rated 50 Indian movies’ data (i.e. name, rating, year of release) and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1bb50cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_50_indian_movies(url):\n",
    "    \n",
    "    ## Send HTTP request to IMDb Top 250 page\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # find all top movies data\n",
    "    top_movies = soup.find_all('td',class_ = \"titleColumn\")\n",
    "    \n",
    "    # extract name rating year for the each movie\n",
    "    \n",
    "    movie_names = []\n",
    "    movie_ratings = []\n",
    "    movie_years = []\n",
    "    \n",
    "    for movies in top_movies[:50]:\n",
    "        \n",
    "        \n",
    "        #extract name\n",
    "        name = movies.a.text\n",
    "        movie_names.append(name)\n",
    "        \n",
    "        #extract rating of the movies\n",
    "         \n",
    "        rating = movies.parent.find(\"td\", class_=\"ratingColumn imdbRating\").strong.text\n",
    "        movie_ratings.append(rating)\n",
    "        \n",
    "        \n",
    "        # extract year of release\n",
    "        year = movies.span.text.strip(\"()\")\n",
    "        movie_years.append(year)\n",
    "    \n",
    "    \n",
    "    # Create DataFrame \n",
    "    df = pd.DataFrame({\n",
    "        'Name':movie_names,\n",
    "        'Rating':movie_ratings,\n",
    "        'Year':movie_years\n",
    "        \n",
    "        })\n",
    "    \n",
    "    return df\n",
    "       \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e564e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Name Rating  Year\n",
      "0   Ramayana: The Legend of Prince Rama    8.6  1993\n",
      "1            Rocketry: The Nambi Effect    8.4  2022\n",
      "2                               Nayakan    8.4  1987\n",
      "3                              Gol Maal    8.4  1979\n",
      "4                           777 Charlie    8.4  2022\n",
      "5                            Anbe Sivam    8.4  2003\n",
      "6                     Pariyerum Perumal    8.4  2018\n",
      "7                              Jai Bhim    8.4  2021\n",
      "8                           Apur Sansar    8.4  1959\n",
      "9                              3 Idiots    8.4  2009\n",
      "10                     Manichitrathazhu    8.3  1993\n",
      "11                                #Home    8.3  2021\n",
      "12                      Soorarai Pottru    8.3  2020\n",
      "13                         Black Friday    8.3  2004\n",
      "14                    Kumbalangi Nights    8.3  2019\n",
      "15                    C/o Kancharapalem    8.3  2018\n",
      "16                     Taare Zameen Par    8.3  2007\n",
      "17                             Kireedam    8.3  1989\n",
      "18                               Dangal    8.3  2016\n",
      "19                               Kaithi    8.3  2019\n",
      "20                               Jersey    8.3  2019\n",
      "21                                   96    8.3  2018\n",
      "22                          Maya Bazaar    8.2  1957\n",
      "23                            Natsamrat    8.2  2016\n",
      "24                           Drishyam 2    8.2  2021\n",
      "25                               Asuran    8.2  2019\n",
      "26                           Sita Ramam    8.2  2022\n",
      "27                         Thevar Magan    8.2  1992\n",
      "28                           Visaaranai    8.2  2015\n",
      "29                  Sarpatta Parambarai    8.2  2021\n",
      "30                           Thalapathi    8.2  1991\n",
      "31                         Nadodikkattu    8.2  1987\n",
      "32                      Pather Panchali    8.2  1955\n",
      "33                             Drishyam    8.2  2013\n",
      "34                         Thani Oruvan    8.2  2015\n",
      "35                   Jaane Bhi Do Yaaro    8.2  1983\n",
      "36                         Vada Chennai    8.2  2018\n",
      "37                            Aparajito    8.2  1956\n",
      "38                         Sardar Udham    8.2  2021\n",
      "39                    Khosla Ka Ghosla!    8.2  2006\n",
      "40                              Anniyan    8.2  2005\n",
      "41                             Ratsasan    8.1  2018\n",
      "42                        Chupke Chupke    8.1  1975\n",
      "43                   Gangs of Wasseypur    8.1  2012\n",
      "44                             Drishyam    8.1  2015\n",
      "45                              Peranbu    8.1  2018\n",
      "46                             Mahanati    8.1  2018\n",
      "47                       Bangalore Days    8.1  2014\n",
      "48                                Satya    8.1  1998\n",
      "49                               Premam    8.1  2015\n"
     ]
    }
   ],
   "source": [
    "top_indian_movies = get_top_50_indian_movies('https://www.imdb.com/india/top-rated-indian-movies/')\n",
    "print(top_indian_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c49ab",
   "metadata": {},
   "source": [
    "## 4) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) from https://presidentofindia.nic.in/former-presidents.htm and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ba0c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function that takes in a URL as input\n",
    "def termofoffice(url):\n",
    "    \n",
    "    # Getting response from the URL\n",
    "    req = requests.get(url)\n",
    "    \n",
    "    # Parsing the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(req.content,'html.parser')\n",
    "    \n",
    "    # Creating an empty list to store the names of presidents\n",
    "    Name=[]\n",
    "    # Finding all the president listings and appending to the Name list\n",
    "    for i in soup.find_all('div','presidentListing'):\n",
    "        Name.append(i.h3.text.strip())\n",
    "\n",
    "    # Creating an empty list to store the terms of office for each president\n",
    "    term=[]\n",
    "    # Finding all the terms of office and appending to the term list\n",
    "    for i in soup.find_all('div','presidentListing'):\n",
    "        term.append(i.p.text)\n",
    "    \n",
    "    # Creating a pandas DataFrame to store the data\n",
    "    df = pd.DataFrame({'Name':Name,\n",
    "                      'Term Of Office':term})\n",
    "    \n",
    "    # Returning the DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c208124d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Term Of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind (birth - 1945)</td>\n",
       "      <td>Term of Office: 25 July, 2017 to 25 July, 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee (1935-2020)</td>\n",
       "      <td>Term of Office: 25 July, 2012 to 25 July, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil (birth - 1934)</td>\n",
       "      <td>Term of Office: 25 July, 2007 to 25 July, 2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam (1931-2015)</td>\n",
       "      <td>Term of Office: 25 July, 2002 to 25 July, 2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan (1920 - 2005)</td>\n",
       "      <td>Term of Office: 25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma (1918-1999)</td>\n",
       "      <td>Term of Office: 25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman (1910-2009)</td>\n",
       "      <td>Term of Office: 25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh (1916-1994)</td>\n",
       "      <td>Term of Office: 25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy (1913-1996)</td>\n",
       "      <td>Term of Office: 25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed (1905-1977)</td>\n",
       "      <td>Term of Office: 24 August, 1974 to 11 February...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri (1894-1980)</td>\n",
       "      <td>Term of Office: 3 May, 1969 to 20 July, 1969 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain (1897-1969)</td>\n",
       "      <td>Term of Office: 13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan (1888-1975)</td>\n",
       "      <td>Term of Office: 13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad (1884-1963)</td>\n",
       "      <td>Term of Office: 26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Name  \\\n",
       "0           Shri Ram Nath Kovind (birth - 1945)   \n",
       "1             Shri Pranab Mukherjee (1935-2020)   \n",
       "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
       "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
       "4            Shri K. R. Narayanan (1920 - 2005)   \n",
       "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
       "6               Shri R Venkataraman (1910-2009)   \n",
       "7                  Giani Zail Singh (1916-1994)   \n",
       "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
       "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
       "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
       "11                 Dr. Zakir Husain (1897-1969)   \n",
       "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
       "13              Dr. Rajendra Prasad (1884-1963)   \n",
       "\n",
       "                                       Term Of Office  \n",
       "0     Term of Office: 25 July, 2017 to 25 July, 2022   \n",
       "1     Term of Office: 25 July, 2012 to 25 July, 2017   \n",
       "2     Term of Office: 25 July, 2007 to 25 July, 2012   \n",
       "3     Term of Office: 25 July, 2002 to 25 July, 2007   \n",
       "4     Term of Office: 25 July, 1997 to 25 July, 2002   \n",
       "5     Term of Office: 25 July, 1992 to 25 July, 1997   \n",
       "6     Term of Office: 25 July, 1987 to 25 July, 1992   \n",
       "7     Term of Office: 25 July, 1982 to 25 July, 1987   \n",
       "8     Term of Office: 25 July, 1977 to 25 July, 1982   \n",
       "9   Term of Office: 24 August, 1974 to 11 February...  \n",
       "10  Term of Office: 3 May, 1969 to 20 July, 1969 a...  \n",
       "11        Term of Office: 13 May, 1967 to 3 May, 1969  \n",
       "12       Term of Office: 13 May, 1962 to 13 May, 1967  \n",
       "13   Term of Office: 26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pres_office = termofoffice('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "pres_office"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374e64d",
   "metadata": {},
   "source": [
    "## 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96658a9a",
   "metadata": {},
   "source": [
    "###  a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14ed5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "\n",
    "\n",
    "def teams_(url):\n",
    "\n",
    "    # Send a GET request to the URL and get the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the response status code is 200 (i.e., OK)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the table containing the rankings using its class attribute\n",
    "        table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "\n",
    "        # Check if the table is found\n",
    "        if table is not None:\n",
    "            # Find all rows in the table except for the first row (header row)\n",
    "            rows = table.find_all(\"tr\")[1:11]\n",
    "\n",
    "            # Initialize an empty list to store the data for each team\n",
    "            data = []\n",
    "\n",
    "            # Loop through each row and extract the team name, matches, points, and rating\n",
    "            for row in rows:\n",
    "                # Find all cells in the row\n",
    "                cells = row.find_all(\"td\")\n",
    "\n",
    "                # Extract the team name from the second cell\n",
    "                team = cells[1].find(\"span\", {\"class\": \"u-hide-phablet\"}).text.strip()\n",
    "\n",
    "                # Extract the number of matches from the third cell\n",
    "                matches = cells[2].text.strip()\n",
    "\n",
    "                # Extract the number of points from the fourth cell\n",
    "                points = cells[3].text.strip()\n",
    "\n",
    "                # Extract the rating from the fifth cell\n",
    "                rating = cells[4].text.strip()\n",
    "\n",
    "                # Add the data for this team to the list\n",
    "                data.append([team, matches, points, rating])\n",
    "\n",
    "            # Convert the list of data to a pandas DataFrame\n",
    "            df = pd.DataFrame(data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "\n",
    "            # Print the DataFrame to display the top 10 ODI teams in men's cricket along with their matches, points, and rating\n",
    "            return(df)\n",
    "        else:\n",
    "            # If the table is not found, print an error message\n",
    "            print(\"Table not found.\")\n",
    "    else:\n",
    "        # If the page is not fetched successfully, print an error message\n",
    "        print(\"Failed to fetch page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56650c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Team Matches  Points Rating\n",
      "0         India      69  18,445    267\n",
      "1       England      52  13,555    261\n",
      "2      Pakistan      58  14,775    255\n",
      "3  South Africa      44  11,170    254\n",
      "4   New Zealand      53  13,371    252\n",
      "5     Australia      47  11,784    251\n",
      "6   West Indies      54  12,855    238\n",
      "7     Sri Lanka      53  12,485    236\n",
      "8    Bangladesh      55  12,506    227\n",
      "9   Afghanistan      36   7,928    220\n"
     ]
    }
   ],
   "source": [
    "team = teams_('https://www.icc-cricket.com/rankings/mens/team-rankings/t20i')\n",
    "print(team)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffe39e1",
   "metadata": {},
   "source": [
    "### b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19e51083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_batsmenss(url):\n",
    "    # send request to the server \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # create soup object\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # fetching  the top player data\n",
    "    name = []\n",
    "    team = []\n",
    "    rate =[]\n",
    "    a=soup.find('div', 'rankings-block__banner--name').text\n",
    "    b=soup.find('div','rankings-block__banner--nationality').text[2:5]\n",
    "    c=soup.find('div','rankings-block__banner--rating').text\n",
    "    name.append(a)\n",
    "    team.append(b)\n",
    "    rate.append(c)\n",
    "    # extract the names of the batsmens\n",
    "    namesss = []\n",
    "    for i in soup.find_all('td', 'table-body__cell name'):\n",
    "        namesss.append(i.text.strip())\n",
    "    # extract the teams of  the players\n",
    "    teams = []\n",
    "    for i in soup.find_all('td', 'table-body__cell nationality-logo'):\n",
    "        teams.append(i.text.strip())\n",
    "    # extract the ratings of the batsmens\n",
    "    rating = []\n",
    "    for i in soup.find_all('td', 'table-body__cell u-text-right rating'):\n",
    "        rating.append(i.text.strip())\n",
    "    # adding the data together \n",
    "    names = name + namesss\n",
    "    teams = team + teams\n",
    "    rating = rate + rating\n",
    "    \n",
    "    # create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'Batsmen':names,\n",
    "        'Team':teams,\n",
    "        'Rating':rating\n",
    "    })\n",
    "    \n",
    "    # return the DataFrame\n",
    "    return df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "633fb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Batsmen Team Rating\n",
      "0             Babar Azam  PAK    887\n",
      "1  Rassie van der Dussen   SA    777\n",
      "2            Imam-ul-Haq  PAK    740\n",
      "3        Quinton de Kock   SA    740\n",
      "4           Shubman Gill  IND    738\n",
      "5           David Warner  AUS    726\n",
      "6            Virat Kohli  IND    719\n",
      "7           Rohit Sharma  IND    707\n",
      "8            Steve Smith  AUS    702\n",
      "9           Fakhar Zaman  PAK    699\n"
     ]
    }
   ],
   "source": [
    "name = top_batsmenss('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad281fa",
   "metadata": {},
   "source": [
    "### c) Top 10 ODI bowlers along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab699aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_ODI_bowlers(url):\n",
    "    # Send a request to the ICC ODI rankings web page and get the response\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the top 10 ODI bowlers\n",
    "    table = soup.find('table', attrs={'class': 'table'})\n",
    "\n",
    "    # Extract the data from the table and store it in a list of dictionaries\n",
    "    data = []\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:11]: # We only need the top 10 bowlers\n",
    "        cells = row.find_all('td')\n",
    "        player_name = cells[1].text.strip()\n",
    "        team = cells[2].text.strip()\n",
    "        rating = cells[3].text.strip()\n",
    "        data.append({\n",
    "            'Player Name': player_name,\n",
    "            'Team': team,\n",
    "            'Rating': rating\n",
    "        })\n",
    "\n",
    "    # Create a Pandas DataFrame from the list of dictionaries and return it\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3b868ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Player Name Team Rating\n",
      "0    Josh Hazlewood  AUS    705\n",
      "1       Trent Boult   NZ    701\n",
      "2    Mohammed Siraj  IND    691\n",
      "3    Mitchell Starc  AUS    686\n",
      "4       Rashid Khan  AFG    659\n",
      "5        Adam Zampa  AUS    652\n",
      "6    Shaheen Afridi  PAK    641\n",
      "7  Mujeeb Ur Rahman  AFG    637\n",
      "8   Shakib Al Hasan  BAN    636\n",
      "9        Matt Henry   NZ    635\n"
     ]
    }
   ],
   "source": [
    "top = get_top_10_ODI_bowlers('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "print(top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc012309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a782055f",
   "metadata": {},
   "source": [
    "## 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef7f174",
   "metadata": {},
   "source": [
    "#### a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fb40602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_women_odi(url):\n",
    "    \n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Find all the rows in the table\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Loop through the rows and extract the data\n",
    "    for row in rows[1:11]: # Take only first 10 rows for top 10 teams\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        data.append(cols)\n",
    "\n",
    "    # Create a Pandas data frame with the data\n",
    "    df = pd.DataFrame(data, columns=['Position', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "    # Remove the \\n character from the team names\n",
    "    df['Team'] = df['Team'].replace('\\n', '', regex=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f176453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Position            Team Matches Points Rating\n",
      "0        1    AustraliaAUS      21  3,603    172\n",
      "1        2      EnglandENG      28  3,342    119\n",
      "2        3  South AfricaSA      26  3,098    119\n",
      "3        4        IndiaIND      27  2,820    104\n",
      "4        5   New ZealandNZ      25  2,553    102\n",
      "5        6   West IndiesWI      27  2,535     94\n",
      "6        7   BangladeshBAN      13    983     76\n",
      "7        8     ThailandTHA       8    572     72\n",
      "8        9     PakistanPAK      27  1,678     62\n",
      "9       10     Sri LankaSL       8    353     44\n"
     ]
    }
   ],
   "source": [
    "top_odi_wo = top_women_odi('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "print(top_odi_wo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e074f",
   "metadata": {},
   "source": [
    "#### b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2631567",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_10_women_batting_players(url):\n",
    "    # URL of the ICC women's ODI batting rankings\n",
    "    \n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table rankings-table'})\n",
    "\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Find all the rows in the table\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Loop through the rows and extract the data\n",
    "    for row in rows[1:11]: # Take only first 10 rows for top 10 players\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        player = cols[1]\n",
    "        team = cols[2]\n",
    "        rating = cols[3]\n",
    "        data.append([player, team, rating])\n",
    "\n",
    "    # Create a Pandas data frame with the data\n",
    "    df = pd.DataFrame(data, columns=['Player', 'Team', 'Rating'])\n",
    "\n",
    "    # Add position numbers to the data frame\n",
    "    df.index += 1\n",
    "\n",
    "    # Return the data frame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cd0d89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Player Team Rating\n",
      "1          Alyssa Healy  AUS    762\n",
      "2           Beth Mooney  AUS    754\n",
      "3       Laura Wolvaardt   SA    732\n",
      "4        Natalie Sciver  ENG    731\n",
      "5           Meg Lanning  AUS    717\n",
      "6      Harmanpreet Kaur  IND    716\n",
      "7       Smriti Mandhana  IND    714\n",
      "8        Rachael Haynes  AUS    680\n",
      "9   Chamari Athapaththu   SL    655\n",
      "10    Amy Satterthwaite   NZ    641\n"
     ]
    }
   ],
   "source": [
    "wo_batsmen = get_top_10_women_batting_players('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "print(wo_batsmen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67c522",
   "metadata": {},
   "source": [
    "#### c) Top 10 women’s ODI all-rounder along with the records of their team and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6e8542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_top_10_women_allrounders(url):\n",
    "    # URL of the ICC women's ODI all-rounder rankings\n",
    "   \n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Find the table containing the rankings data\n",
    "    table = soup.find('table', {'class': 'table rankings-table'})\n",
    "\n",
    "    # Create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # Find all the rows in the table\n",
    "    rows = table.find_all('tr')\n",
    "\n",
    "    # Loop through the rows and extract the data\n",
    "    for row in rows[1:11]: # Take only first 10 rows for top 10 players\n",
    "        cols = row.find_all('td')\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        player = cols[1]\n",
    "        team = cols[2]\n",
    "        rating = cols[3]\n",
    "        data.append([player, team, rating])\n",
    "\n",
    "    # Create a Pandas data frame with the data\n",
    "    df = pd.DataFrame(data, columns=['Player', 'Team', 'Rating'])\n",
    "\n",
    "    # Add position numbers to the data frame\n",
    "    df.index += 1\n",
    "\n",
    "    # Return the data frame\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b5b75b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Player</th>\n",
       "      <th>Team</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ellyse Perry</td>\n",
       "      <td>AUS</td>\n",
       "      <td>366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amelia Kerr</td>\n",
       "      <td>NZ</td>\n",
       "      <td>336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepti Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ashleigh Gardner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Nida Dar</td>\n",
       "      <td>PAK</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Player Team Rating\n",
       "1    Hayley Matthews   WI    373\n",
       "2     Natalie Sciver  ENG    371\n",
       "3       Ellyse Perry  AUS    366\n",
       "4     Marizanne Kapp   SA    349\n",
       "5        Amelia Kerr   NZ    336\n",
       "6      Deepti Sharma  IND    322\n",
       "7   Ashleigh Gardner  AUS    292\n",
       "8      Jess Jonassen  AUS    250\n",
       "9           Nida Dar  PAK    232\n",
       "10    Jhulan Goswami  IND    214"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo_all = get_top_10_women_allrounders('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')\n",
    "wo_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead3f865",
   "metadata": {},
   "source": [
    "### 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\u0002\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38b05e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Defining function that takes in a URL as input\n",
    "def news_cnbc(url):\n",
    "    \n",
    "    # Getting response from the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parsing the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Creating an empty list to store the news article timestamps\n",
    "    time=[]\n",
    "    # Finding all the timestamps and appending to the time list\n",
    "    for i in soup.find_all('time','LatestNews-timestamp'):\n",
    "        time.append(i.text)\n",
    "    \n",
    "    # Creating an empty list to store the news article headlines\n",
    "    news=[]\n",
    "    # Finding all the headlines and appending to the news list\n",
    "    for i in soup.find_all('a','LatestNews-headline'):\n",
    "        news.append(i.text)\n",
    "\n",
    "    # Creating an empty list to store the news article links\n",
    "    links=[]\n",
    "    # Finding all the links and appending to the links list\n",
    "    for link in soup.find_all('a','LatestNews-headline'):\n",
    "        links.append(link.get('href'))\n",
    "    \n",
    "    # Creating a pandas DataFrame to store the data\n",
    "    df = pd.DataFrame({'Headline':news,\n",
    "                      'Time':time,\n",
    "                      'News Links':links})\n",
    "    \n",
    "    # Returning the DataFrame\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "859e6630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News Links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>San Francisco Fed leader likely not a major pl...</td>\n",
       "      <td>24 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/san-francisco-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sen. Elizabeth Warren says she wants to make b...</td>\n",
       "      <td>43 Min Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/elizabeth-warr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 in-demand side hustles you can do from home...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/10-in-demand-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's the U.S., not Europe's banking system tha...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/its-the-us-not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Trump faces about 30 criminal counts for docum...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/trump-faces-ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Investors believe the stock market is set for ...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/investors-beli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This biotech can gain 50% on interest in seque...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/this-biotech-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nikola shares sink after $100 million stock of...</td>\n",
       "      <td>1 Hour Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/nikola-stock-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Justice Department sues Norfolk Southern over ...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/justice-depart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'Jeopardy' champ Amy Schneider's new favorite ...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/jeopardy-champ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jim Cramer's top 10 things to watch in the sto...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/cramers-top-th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Wall Street lines up against Gary Gensler on o...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/wall-street-li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Trump indictment: Grand jury heard about hush ...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/trump-indicted...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Signal President Meredith Whittaker learned wh...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/signal-preside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The rules for EV tax credits are about to get ...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/ev-tax-credit-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bill Gates took a 'test ride' in an autonomous...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/watch-bill-gat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Friday's top analyst calls: Boeing, Tesla, Dis...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/top-analyst-ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>How TikTok, ByteDance spent over $13 million o...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/tiktok-bytedan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Key Fed inflation gauge rose 0.3% in February,...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/fed-inflation-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Stocks making the biggest moves premarket: Bed...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/stocks-making-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5 things to know before the stock market opens...</td>\n",
       "      <td>3 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/5-things-to-kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Loop Capital downgrades this Chinese e-commerc...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/loop-capital-d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>China's chip industry will be 'reborn' under U...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/chinas-chip-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Morgan Stanley says this beauty stock is gaini...</td>\n",
       "      <td>4 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Xpeng expands assisted driving tech coverage t...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/xpeng-expands-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Morgan Stanley says this e-commerce stock is a...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/morgan-stanley...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>'Nationalizing bond markets' left central bank...</td>\n",
       "      <td>5 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/nationalizing-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Euro zone price rises cool significantly in Ma...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/inflation-euro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Huawei reports biggest profit decline ever as ...</td>\n",
       "      <td>6 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/huawei-earning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Treasury yields are flat as bond market wraps ...</td>\n",
       "      <td>7 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2023/03/31/us-treasury-yi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline         Time  \\\n",
       "0   San Francisco Fed leader likely not a major pl...   24 Min Ago   \n",
       "1   Sen. Elizabeth Warren says she wants to make b...   43 Min Ago   \n",
       "2   10 in-demand side hustles you can do from home...   1 Hour Ago   \n",
       "3   It's the U.S., not Europe's banking system tha...   1 Hour Ago   \n",
       "4   Trump faces about 30 criminal counts for docum...   1 Hour Ago   \n",
       "5   Investors believe the stock market is set for ...   1 Hour Ago   \n",
       "6   This biotech can gain 50% on interest in seque...   1 Hour Ago   \n",
       "7   Nikola shares sink after $100 million stock of...   1 Hour Ago   \n",
       "8   Justice Department sues Norfolk Southern over ...  2 Hours Ago   \n",
       "9   'Jeopardy' champ Amy Schneider's new favorite ...  2 Hours Ago   \n",
       "10  Jim Cramer's top 10 things to watch in the sto...  2 Hours Ago   \n",
       "11  Wall Street lines up against Gary Gensler on o...  2 Hours Ago   \n",
       "12  Trump indictment: Grand jury heard about hush ...  2 Hours Ago   \n",
       "13  Signal President Meredith Whittaker learned wh...  2 Hours Ago   \n",
       "14  The rules for EV tax credits are about to get ...  2 Hours Ago   \n",
       "15  Bill Gates took a 'test ride' in an autonomous...  2 Hours Ago   \n",
       "16  Friday's top analyst calls: Boeing, Tesla, Dis...  2 Hours Ago   \n",
       "17  How TikTok, ByteDance spent over $13 million o...  2 Hours Ago   \n",
       "18  Key Fed inflation gauge rose 0.3% in February,...  2 Hours Ago   \n",
       "19  Stocks making the biggest moves premarket: Bed...  3 Hours Ago   \n",
       "20  5 things to know before the stock market opens...  3 Hours Ago   \n",
       "21  Loop Capital downgrades this Chinese e-commerc...  4 Hours Ago   \n",
       "22  China's chip industry will be 'reborn' under U...  4 Hours Ago   \n",
       "23  Morgan Stanley says this beauty stock is gaini...  4 Hours Ago   \n",
       "24  Xpeng expands assisted driving tech coverage t...  5 Hours Ago   \n",
       "25  Morgan Stanley says this e-commerce stock is a...  5 Hours Ago   \n",
       "26  'Nationalizing bond markets' left central bank...  5 Hours Ago   \n",
       "27  Euro zone price rises cool significantly in Ma...  6 Hours Ago   \n",
       "28  Huawei reports biggest profit decline ever as ...  6 Hours Ago   \n",
       "29  Treasury yields are flat as bond market wraps ...  7 Hours Ago   \n",
       "\n",
       "                                           News Links  \n",
       "0   https://www.cnbc.com/2023/03/31/san-francisco-...  \n",
       "1   https://www.cnbc.com/2023/03/31/elizabeth-warr...  \n",
       "2   https://www.cnbc.com/2023/03/31/10-in-demand-r...  \n",
       "3   https://www.cnbc.com/2023/03/31/its-the-us-not...  \n",
       "4   https://www.cnbc.com/2023/03/31/trump-faces-ab...  \n",
       "5   https://www.cnbc.com/2023/03/31/investors-beli...  \n",
       "6   https://www.cnbc.com/2023/03/31/this-biotech-c...  \n",
       "7   https://www.cnbc.com/2023/03/31/nikola-stock-s...  \n",
       "8   https://www.cnbc.com/2023/03/31/justice-depart...  \n",
       "9   https://www.cnbc.com/2023/03/31/jeopardy-champ...  \n",
       "10  https://www.cnbc.com/2023/03/31/cramers-top-th...  \n",
       "11  https://www.cnbc.com/2023/03/31/wall-street-li...  \n",
       "12  https://www.cnbc.com/2023/03/31/trump-indicted...  \n",
       "13  https://www.cnbc.com/2023/03/31/signal-preside...  \n",
       "14  https://www.cnbc.com/2023/03/31/ev-tax-credit-...  \n",
       "15  https://www.cnbc.com/2023/03/31/watch-bill-gat...  \n",
       "16  https://www.cnbc.com/2023/03/31/top-analyst-ca...  \n",
       "17  https://www.cnbc.com/2023/03/31/tiktok-bytedan...  \n",
       "18  https://www.cnbc.com/2023/03/31/fed-inflation-...  \n",
       "19  https://www.cnbc.com/2023/03/31/stocks-making-...  \n",
       "20  https://www.cnbc.com/2023/03/31/5-things-to-kn...  \n",
       "21  https://www.cnbc.com/2023/03/31/loop-capital-d...  \n",
       "22  https://www.cnbc.com/2023/03/31/chinas-chip-in...  \n",
       "23  https://www.cnbc.com/2023/03/31/morgan-stanley...  \n",
       "24  https://www.cnbc.com/2023/03/31/xpeng-expands-...  \n",
       "25  https://www.cnbc.com/2023/03/31/morgan-stanley...  \n",
       "26  https://www.cnbc.com/2023/03/31/nationalizing-...  \n",
       "27  https://www.cnbc.com/2023/03/31/inflation-euro...  \n",
       "28  https://www.cnbc.com/2023/03/31/huawei-earning...  \n",
       "29  https://www.cnbc.com/2023/03/31/us-treasury-yi...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = news_cnbc('https://www.cnbc.com/world/?region=world')\n",
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab03c78",
   "metadata": {},
   "source": [
    "### 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1265bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articless(url):\n",
    "    # Send a GET request to the provided URL and get the HTML content\n",
    "    response=requests.get(url)\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "    # Get the paper titles by finding all 'h2' tags with specific class\n",
    "    text =[]\n",
    "    for i in soup.find_all('h2','sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg'):\n",
    "        text.append(i.text)\n",
    "\n",
    "    # Get the author names by finding all 'span' tags with specific class\n",
    "    author = []\n",
    "    for i in soup.find_all('span','sc-1w3fpd7-0 dnCnAO'):\n",
    "        author.append(i.text)\n",
    "\n",
    "    # Get the published dates by finding all 'span' tags with specific class\n",
    "    date =[]\n",
    "    for i in soup.find_all('span','sc-1thf9ly-2 dvggWt'):\n",
    "        date.append(i.text)\n",
    "\n",
    "    # Get the paper URLs by finding all 'a' tags with specific class\n",
    "    links=[]\n",
    "    for link in soup.findAll('a','sc-5smygv-0 fIXTHm'):\n",
    "        links.append(link.get('href'))\n",
    "    \n",
    "    # Check if all lists have the same length, raise ValueError if not\n",
    "    if len(set(map(len, (text, author, date, links)))) > 1:\n",
    "        raise ValueError(\"Scraped data has different lengths\")\n",
    "\n",
    "    # Create a pandas DataFrame with the collected data and return it\n",
    "    df= pd.DataFrame({'Paper Title':text,\n",
    "                     'Authors': author,\n",
    "                     'Published Date':date,\n",
    "                     'Paper URL': links})\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7793c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = get_articless('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "articles "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b193f4",
   "metadata": {},
   "source": [
    "### 9) Write a python program to scrape mentioned details from dineout.co.in and make data frame\u0002\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de98f5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def restaurant_data(url):\n",
    "    # Send a GET request to the provided URL and get the HTML content\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content,'html.parser')\n",
    "\n",
    "    # Get the restaurant names by finding all 'a' tags with class 'restnt-name ellipsis'\n",
    "    res =[]\n",
    "    for i in soup.find_all('a','restnt-name ellipsis'):\n",
    "        res.append(i.text.strip())\n",
    "\n",
    "    # Get the cuisine by finding all 'span' tags with class 'double-line-ellipsis',\n",
    "    # splitting the text by '|' and taking the second element\n",
    "    cuisine =[]\n",
    "    for i in soup.find_all('span','double-line-ellipsis'):\n",
    "        cuisine.append(i.text.split(\"|\")[1])\n",
    "        \n",
    "    # Get the location by finding all 'div' tags with class 'restnt-loc ellipsis'\n",
    "    location =[]\n",
    "    for i in soup.find_all('div','restnt-loc ellipsis'):\n",
    "        location.append(i.text)\n",
    "\n",
    "    # Get the rating by finding all 'div' tags with class 'restnt-rating rating-4'\n",
    "    rating=[]\n",
    "    for i in soup.find_all('div',\"restnt-rating rating-4\"):\n",
    "        rating.append(i.text)\n",
    "\n",
    "    # Get the image URLs by finding all 'img' tags with class 'no-img'\n",
    "    # and extracting the 'data-src' attribute\n",
    "    images=[]\n",
    "    for img in soup.findAll('img','no-img'):\n",
    "        images.append(img.get('data-src'))\n",
    "    \n",
    "    # Create a pandas DataFrame with the collected data and return it\n",
    "    df = pd.DataFrame({'Restaurant Name':res,\n",
    "                      'Cuisine':cuisine,\n",
    "                      'Location':location,\n",
    "                      'Ratings':rating,\n",
    "                      'Image URL':images})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1060569b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant Name</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Image URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.7</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Restaurant Name                        Cuisine  \\\n",
       "0                   Castle Barbeque          Chinese, North Indian   \n",
       "1                   Jungle Jamboree   North Indian, Asian, Italian   \n",
       "2                        Cafe Knosh           Italian, Continental   \n",
       "3                   Castle Barbeque          Chinese, North Indian   \n",
       "4              The Barbeque Company          North Indian, Chinese   \n",
       "5                       India Grill          North Indian, Italian   \n",
       "6                    Delhi Barbeque                   North Indian   \n",
       "7  The Monarch - Bar Be Que Village                   North Indian   \n",
       "8                 Indian Grill Room          North Indian, Mughlai   \n",
       "\n",
       "                                            Location Ratings  \\\n",
       "0                     Connaught Place, Central Delhi     4.1   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "2  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "3             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "4                 Gardens Galleria,Sector 38A, Noida     3.9   \n",
       "5               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.7   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "\n",
       "                                           Image URL  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant = restaurant_data('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e752283",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
