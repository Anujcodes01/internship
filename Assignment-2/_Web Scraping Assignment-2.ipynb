{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff73663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: selenium in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (4.8.3)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.11)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (2022.9.14)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba06792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import StaleElementReferenceException ,NoSuchElementException\n",
    "import time \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d71af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94450f5e",
   "metadata": {},
   "source": [
    "### Q1: Write a python program to scrape data for ‚ÄúData Analyst‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Analyst‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the \n",
    "location‚Äù field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5afe229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri.com page on automated chrome browser\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13812286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the designation as required in the question\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys(\"Data Analyst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e36461ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the  location as required in the question\n",
    "location = driver.find_element(By.XPATH,'/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input')\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "367d547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on search button to search the required data \n",
    "search = driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4609237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job-title</th>\n",
       "      <th>job-location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Analyst III</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Walmart</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Clarivate</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst || Bangalore || Male/ Female || 1...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>TeamLease</td>\n",
       "      <td>1-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Ingersoll Rand</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst - EdTech</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Talentstack</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Unusual Hire</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Engineer/Data Analyst</td>\n",
       "      <td>Hybrid - Bangalore/Bengaluru, Kolkata, Hyderab...</td>\n",
       "      <td>Tech Mahindra</td>\n",
       "      <td>6-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst - Contractual</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Search Advisers Services Guj</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst - Contractual</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Search Advisers Services Guj</td>\n",
       "      <td>2-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst - MySQL</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Talentstack</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job-title  \\\n",
       "0                                   Data Analyst III   \n",
       "1                                       Data Analyst   \n",
       "2  Data Analyst || Bangalore || Male/ Female || 1...   \n",
       "3                                       Data Analyst   \n",
       "4                              Data Analyst - EdTech   \n",
       "5                                       Data Analyst   \n",
       "6                         Data Engineer/Data Analyst   \n",
       "7                         Data Analyst - Contractual   \n",
       "8                         Data Analyst - Contractual   \n",
       "9                               Data Analyst - MySQL   \n",
       "\n",
       "                                        job-location  \\\n",
       "0                                Bangalore/Bengaluru   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6  Hybrid - Bangalore/Bengaluru, Kolkata, Hyderab...   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                   company_name experience_required  \n",
       "0                       Walmart             3-7 Yrs  \n",
       "1                     Clarivate             2-4 Yrs  \n",
       "2                     TeamLease             1-5 Yrs  \n",
       "3                Ingersoll Rand             3-6 Yrs  \n",
       "4                   Talentstack             2-6 Yrs  \n",
       "5                  Unusual Hire             1-4 Yrs  \n",
       "6                 Tech Mahindra            6-11 Yrs  \n",
       "7  Search Advisers Services Guj             2-3 Yrs  \n",
       "8  Search Advisers Services Guj             2-3 Yrs  \n",
       "9                   Talentstack             2-7 Yrs  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty lists to store the scraped data\n",
    "job_titles = []\n",
    "company_name = []\n",
    "job_location = []\n",
    "experience_required = []\n",
    "\n",
    "# scrape job titles of first 10 jobs\n",
    "job_tags = driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in job_tags[:10]:\n",
    "    job= i.text\n",
    "    job_titles.append(job)\n",
    "\n",
    "# scrape company names of first 10 jobs\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[:10]:\n",
    "    company= i.text\n",
    "    company_name.append(company)\n",
    "\n",
    "# scrape job locations of first 10 jobs\n",
    "location_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags[:10]:\n",
    "    location = i.text\n",
    "    job_location.append(location)\n",
    "\n",
    "# scrape experience required for first 10 jobs\n",
    "experience_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for i in experience_tags[:10]:\n",
    "    experience= i.text\n",
    "    experience_required.append(experience)\n",
    "\n",
    "# create pandas dataframe from the scraped data\n",
    "df = pd.DataFrame({'job-title':job_titles,\n",
    "                   'job-location':job_location, \n",
    "                   'company_name':company_name, \n",
    "                   'experience_required':experience_required})\n",
    "\n",
    "# print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448c27b",
   "metadata": {},
   "source": [
    "### Q2:Write a python program to scrape data for ‚ÄúData Scientist‚Äù Job position in ‚ÄúBangalore‚Äù location. You have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, Companies‚Äù field and enter ‚ÄúBangalore‚Äù in ‚Äúenter the \n",
    "location‚Äù field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then scrape the data for the first 10 jobs results youget.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93e072e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb51bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri.com page on automated chrome browser\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "597c8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the designation as required in the question\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44bd0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the  location as required in the question\n",
    "location = driver.find_element(By.XPATH,'/html/body/div[1]/div[7]/div/div/div[5]/div/div/div/div[1]/div/input')\n",
    "location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e685ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on search button to search the required data \n",
    "search = driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d022e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job-title</th>\n",
       "      <th>job-location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Science Professional - IBM SPSS Statistic...</td>\n",
       "      <td>Bangalore/Bengaluru, Noida, Mumbai, Pune, Chennai</td>\n",
       "      <td>Hexaware Technologies</td>\n",
       "      <td>5-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analystics &amp; Modeling Specialist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior data scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist_NLP</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>5-11 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Machine Learning (AI) Architect</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>Persistent</td>\n",
       "      <td>5-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Manager - Innovations Hub - Machine Learning</td>\n",
       "      <td>Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...</td>\n",
       "      <td>PwC</td>\n",
       "      <td>7-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>Skillety</td>\n",
       "      <td>7-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...</td>\n",
       "      <td>Clarifai</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job-title  \\\n",
       "0  Data Science Professional - IBM SPSS Statistic...   \n",
       "1                            Data Science Specialist   \n",
       "2                   Analystics & Modeling Specialist   \n",
       "3                              Senior data scientist   \n",
       "4                                 Data Scientist_NLP   \n",
       "5                                     Data Scientist   \n",
       "6                    Machine Learning (AI) Architect   \n",
       "7       Manager - Innovations Hub - Machine Learning   \n",
       "8                                       Data Science   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                        job-location           company_name  \\\n",
       "0  Bangalore/Bengaluru, Noida, Mumbai, Pune, Chennai  Hexaware Technologies   \n",
       "1  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...              Accenture   \n",
       "2  Bangalore/Bengaluru, Kolkata, Mumbai, Hyderaba...              Accenture   \n",
       "3  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...      Fractal Analytics   \n",
       "4  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...      Fractal Analytics   \n",
       "5  Bangalore/Bengaluru, Mumbai, Pune, Chennai, Gu...      Fractal Analytics   \n",
       "6  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...             Persistent   \n",
       "7  Bangalore/Bengaluru, Mumbai, Hyderabad/Secunde...                    PwC   \n",
       "8  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...               Skillety   \n",
       "9  Bangalore/Bengaluru, Kolkata, Mumbai, New Delh...               Clarifai   \n",
       "\n",
       "  experience_required  \n",
       "0             5-8 Yrs  \n",
       "1             2-4 Yrs  \n",
       "2             6-8 Yrs  \n",
       "3             4-8 Yrs  \n",
       "4            5-11 Yrs  \n",
       "5             3-7 Yrs  \n",
       "6            5-12 Yrs  \n",
       "7             7-9 Yrs  \n",
       "8            7-10 Yrs  \n",
       "9             3-7 Yrs  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty lists to store the scraped data\n",
    "data_job = []\n",
    "company__ = []\n",
    "company_location = []\n",
    "experience__ = []\n",
    "\n",
    "# scrape job titles of first 10 jobs\n",
    "job_tags = driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in job_tags[:10]:\n",
    "    job= i.text\n",
    "    data_job.append(job)\n",
    "\n",
    "# scrape company names of first 10 jobs\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[:10]:\n",
    "    company= i.text\n",
    "    company__.append(company)\n",
    "\n",
    "# scrape job locations of first 10 jobs\n",
    "location_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags[:10]:\n",
    "    location = i.text\n",
    "    company_location.append(location)\n",
    "\n",
    "# scrape experience required for first 10 jobs\n",
    "experience_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for i in experience_tags[:10]:\n",
    "    experience= i.text\n",
    "    experience__.append(experience)\n",
    "\n",
    "# create pandas dataframe from the scraped data\n",
    "df = pd.DataFrame({'job-title':data_job,\n",
    "                   'job-location':company_location, \n",
    "                   'company_name':company__, \n",
    "                   'experience_required':experience__})\n",
    "\n",
    "# print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae01e8",
   "metadata": {},
   "source": [
    "### Q3: In this question you have to scrape data using the filters available on the webpage as shown below:ASSIGNMENT 2\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for ‚ÄúData Scientist‚Äù designation for first 10 job results. \n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is ‚ÄúDelhi/NCR‚Äù. The salary filter to be used is ‚Äú3-6‚Äù lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get thewebpage https://www.naukri.com/\n",
    "2. Enter ‚ÄúData Scientist‚Äù in ‚ÄúSkill, Designations, and Companies‚Äù field.\n",
    "3. Then click the searchbutton.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results youget.\n",
    "6. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480b9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec0bbff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the naukri.com page on automated chrome browser\n",
    "driver.get('https://www.naukri.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb000807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the designation as required in the question\n",
    "designation = driver.find_element(By.CLASS_NAME,\"suggestor-input \")\n",
    "designation.send_keys(\"Data Scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca481e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on search button to search the required data \n",
    "search = driver.find_element(By.CLASS_NAME,'qsbSubmit')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c6cda8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the location \n",
    "locationss = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[5]/div[2]/div[2]/label/p/span[1]')\n",
    "locationss.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dfb92be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the salary\n",
    "salary = driver.find_element(By.XPATH,'/html/body/div[1]/div[4]/div/div/section[1]/div[2]/div[6]/div[2]/div[2]/label/p/span[1]')\n",
    "salary.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4051329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job-title</th>\n",
       "      <th>job-location</th>\n",
       "      <th>company_name</th>\n",
       "      <th>experience_required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Temp. WFH - Kochi/Cochin, Kolkata, Hyderabad/S...</td>\n",
       "      <td>Cognizant</td>\n",
       "      <td>6-10 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...</td>\n",
       "      <td>Analytos</td>\n",
       "      <td>0-2 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>Blackbuck</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...</td>\n",
       "      <td>Tabsquare</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...</td>\n",
       "      <td>Analytos</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, United States (USA), Bulgaria</td>\n",
       "      <td>Adidas</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Analyst-Data Science</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>AMERICAN EXPRESS</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Analyst-Data Science</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>AMERICAN EXPRESS</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Hyderabad/Secunderabad, Chenani, Gurgaon/Gurug...</td>\n",
       "      <td>RecruitEForU</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Assistant Manager/Senior Manager - Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>Huquo Consulting Pvt. Ltd</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           job-title  \\\n",
       "0                                     Data Scientist   \n",
       "1                              Junior Data Scientist   \n",
       "2                                     Data Scientist   \n",
       "3                                     Data Scientist   \n",
       "4                                     Data Scientist   \n",
       "5                              Junior Data Scientist   \n",
       "6                               Analyst-Data Science   \n",
       "7                               Analyst-Data Science   \n",
       "8                              Senior Data Scientist   \n",
       "9  Assistant Manager/Senior Manager - Data Scientist   \n",
       "\n",
       "                                        job-location  \\\n",
       "0  Temp. WFH - Kochi/Cochin, Kolkata, Hyderabad/S...   \n",
       "1  Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...   \n",
       "2              Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "3  Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...   \n",
       "4  Kolkata, Mumbai, New Delhi, Hyderabad/Secunder...   \n",
       "5    Gurgaon/Gurugram, United States (USA), Bulgaria   \n",
       "6                                   Gurgaon/Gurugram   \n",
       "7                                   Gurgaon/Gurugram   \n",
       "8  Hyderabad/Secunderabad, Chenani, Gurgaon/Gurug...   \n",
       "9              Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "\n",
       "                company_name experience_required  \n",
       "0                  Cognizant            6-10 Yrs  \n",
       "1                   Analytos             0-2 Yrs  \n",
       "2                  Blackbuck             3-7 Yrs  \n",
       "3                  Tabsquare             1-3 Yrs  \n",
       "4                   Analytos             2-4 Yrs  \n",
       "5                     Adidas             1-6 Yrs  \n",
       "6           AMERICAN EXPRESS             0-3 Yrs  \n",
       "7           AMERICAN EXPRESS             0-3 Yrs  \n",
       "8               RecruitEForU             3-8 Yrs  \n",
       "9  Huquo Consulting Pvt. Ltd             2-7 Yrs  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty lists to store the scraped data\n",
    "datascience_job = []\n",
    "name_company__ = []\n",
    "company_locations = []\n",
    "experience__req= []\n",
    "\n",
    "# scrape job titles of first 10 jobs\n",
    "job_tags = driver.find_elements(By.XPATH,'//a[@class=\"title ellipsis\"]')\n",
    "for i in job_tags[:10]:\n",
    "    job= i.text\n",
    "    datascience_job.append(job)\n",
    "\n",
    "# scrape company names of first 10 jobs\n",
    "company_tags = driver.find_elements(By.XPATH,'//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in company_tags[:10]:\n",
    "    company= i.text\n",
    "    name_company__.append(company)\n",
    "\n",
    "# scrape job locations of first 10 jobs\n",
    "location_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft locWdth\"]')\n",
    "for i in location_tags[:10]:\n",
    "    location = i.text\n",
    "    company_locations.append(location)\n",
    "\n",
    "# scrape experience required for first 10 jobs\n",
    "experience_tags = driver.find_elements(By.XPATH,'//span[@class=\"ellipsis fleft expwdth\"]')\n",
    "for i in experience_tags[:10]:\n",
    "    experience= i.text\n",
    "    experience__req.append(experience)\n",
    "\n",
    "# create pandas dataframe from the scraped data\n",
    "df = pd.DataFrame({'job-title':datascience_job,\n",
    "                   'job-location':company_locations, \n",
    "                   'company_name':name_company__, \n",
    "                   'experience_required':experience__req})\n",
    "\n",
    "# print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f12ac8",
   "metadata": {},
   "source": [
    "### Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url :https://www.flipkart.com/\n",
    "2. Enter ‚Äúsunglasses‚Äù in the search field where ‚Äúsearch for products, brands and more‚Äù is written and \n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the \n",
    "required data asusual.\n",
    "4. After scraping data from the first page, go to the ‚ÄúNext‚Äù Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page asusual\n",
    "6. Repeat this until you get data for 100sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f414fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4e234a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart.com page on automated chrome browser\n",
    "driver.get('https://www.flipkart.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c4cb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the product as required in the question\n",
    "glasses = driver.find_element(By.CLASS_NAME,\"_3704LK \")\n",
    "glasses.send_keys(\"sunglasses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d91d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on search button to search the required data \n",
    "search = driver.find_element(By.CLASS_NAME,'_34RNph')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d72643af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the required data\n",
    "Brand=[]\n",
    "productdescription=[]\n",
    "price=[]\n",
    "\n",
    "# Define the start and end page numbers\n",
    "start = 0\n",
    "end = 3\n",
    "\n",
    "# Loop through the pages\n",
    "for page in range(start, end):\n",
    "    # Find all the elements for Brand tag and append their text to the Brand list\n",
    "    Brand_tag = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in Brand_tag:\n",
    "        Brand.append(i.text)\n",
    "    \n",
    "    # Find all the elements for Product tag and append their text to the productdescription list\n",
    "    product_tag = driver.find_elements(By.XPATH,'//a[@class=\"IRpwTa\"]')\n",
    "    for i in product_tag:\n",
    "        productdescription.append(i.text)\n",
    "    \n",
    "    # Find all the elements for Price tag and append their text to the price list\n",
    "    price_tag = driver.find_elements(By.XPATH,'//div[@class=\"_30jeq3\"]')\n",
    "    for i in price_tag:\n",
    "        price.append(i.text)\n",
    "    \n",
    "    # Find the next button and click on it\n",
    "    nextbutton = driver.find_element(By.CLASS_NAME,'_1LKTO3')\n",
    "    nextbutton.click()\n",
    "    # Wait for 3 seconds for the next page to load\n",
    "    time.sleep(3)\n",
    "    \n",
    "# Get only the first 100 items from each of the lists\n",
    "br = Brand[:100]\n",
    "pro = productdescription[:100]\n",
    "pr = price[:100]\n",
    "\n",
    "# Create a pandas dataframe using the data obtained and store it in a variable named df_sneak\n",
    "df_sneak = pd.DataFrame({\"Brand\":br,\n",
    "                  \"ProductDescription\":pro,\n",
    "                  \"Price\":pr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f31aa0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>ProductDescription</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fair-x</td>\n",
       "      <td>UV Protection Aviator Sunglasses (58)</td>\n",
       "      <td>‚Çπ207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (53)</td>\n",
       "      <td>‚Çπ149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SRPM</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (50)</td>\n",
       "      <td>‚Çπ149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lee Topper</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Polarized Rectangular Sunglasses (60)</td>\n",
       "      <td>‚Çπ664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>SUNBEE</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>‚Çπ179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Clubmaster Sunglasses (54)</td>\n",
       "      <td>‚Çπ224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (53)</td>\n",
       "      <td>‚Çπ179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ROYAL SON</td>\n",
       "      <td>Mirrored Aviator Sunglasses (55)</td>\n",
       "      <td>‚Çπ335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Brand                                ProductDescription Price\n",
       "0   Silver Kartz     UV Protection Wayfarer Sunglasses (Free Size)  ‚Çπ232\n",
       "1         Fair-x             UV Protection Aviator Sunglasses (58)  ‚Çπ207\n",
       "2      Elligator               UV Protection Round Sunglasses (53)  ‚Çπ149\n",
       "3           SRPM            UV Protection Wayfarer Sunglasses (50)  ‚Çπ149\n",
       "4     Lee Topper  UV Protection Rectangular Sunglasses (Free Size)  ‚Çπ219\n",
       "..           ...                                               ...   ...\n",
       "95     ROYAL SON             Polarized Rectangular Sunglasses (60)  ‚Çπ664\n",
       "96        SUNBEE  UV Protection Rectangular Sunglasses (Free Size)  ‚Çπ179\n",
       "97        PIRASO          UV Protection Clubmaster Sunglasses (54)  ‚Çπ224\n",
       "98     Elligator            UV Protection Wayfarer Sunglasses (53)  ‚Çπ179\n",
       "99     ROYAL SON                  Mirrored Aviator Sunglasses (55)  ‚Çπ335\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sneak"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9187b5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: \n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product\u0002reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&market \n",
    "place=FLIPKART\n",
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa57b7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bade18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart.com  iphone 11 page on automated chrome browser\n",
    "driver.get('https://www.flipkart.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "200f260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the product as required in the question\n",
    "iphone = driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "iphone.send_keys(\"iphone11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12245d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on search button to search the required data \n",
    "search = driver.find_element(By.CLASS_NAME,'L0Z3Pu')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2029237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on iphone 11\n",
    "enter= driver.find_element(By.XPATH,'/html/body/div/div/div[3]/div[1]/div[2]/div[4]/div/div/div/a/div[2]/div[1]/div[1]')\n",
    "enter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad8f1d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Very poor</td>\n",
       "      <td>1)This product totally chif quality and wast o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Nice product</td>\n",
       "      <td>Good product on this price range.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Classic !!!!</td>\n",
       "      <td>1.You never gonna feel the quality of this mob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>Still using it in 2019 üòÇ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>Fast performance to previous iPhone x\\nGood ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>4</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Awesome product ‚ù§Ô∏è‚ù§Ô∏è\\nThank you Flipkart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Good quality product</td>\n",
       "      <td>impressively Nice......\\nOne of the greatest i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Nice products thanks flkat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NA</td>\n",
       "      <td>Iphone 5S review</td>\n",
       "      <td>Well the iPhone 5S is here ! its the best iPho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>NA</td>\n",
       "      <td>Worthless</td>\n",
       "      <td>There was defect in camera lens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating        Review Summary  \\\n",
       "0       4             Very poor   \n",
       "1       5          Nice product   \n",
       "2       5          Classic !!!!   \n",
       "3       5             Must buy!   \n",
       "4       5             Fabulous!   \n",
       "..    ...                   ...   \n",
       "95      4         Great product   \n",
       "96      5  Good quality product   \n",
       "97      5      Perfect product!   \n",
       "98     NA      Iphone 5S review   \n",
       "99     NA             Worthless   \n",
       "\n",
       "                                          Full Review  \n",
       "0   1)This product totally chif quality and wast o...  \n",
       "1                   Good product on this price range.  \n",
       "2   1.You never gonna feel the quality of this mob...  \n",
       "3                            Still using it in 2019 üòÇ  \n",
       "4   Fast performance to previous iPhone x\\nGood ca...  \n",
       "..                                                ...  \n",
       "95           Awesome product ‚ù§Ô∏è‚ù§Ô∏è\\nThank you Flipkart  \n",
       "96  impressively Nice......\\nOne of the greatest i...  \n",
       "97                         Nice products thanks flkat  \n",
       "98  Well the iPhone 5S is here ! its the best iPho...  \n",
       "99                    There was defect in camera lens  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the itertools module\n",
    "import itertools\n",
    "\n",
    "# Create empty lists to store the data\n",
    "ratings = []\n",
    "review_summary = []\n",
    "full_review = []\n",
    "\n",
    "# Set the start and end pages to scrape\n",
    "start = 0\n",
    "end = 10\n",
    "\n",
    "# Loop through the pages and scrape data\n",
    "for page in range(start, end):\n",
    "    # Find all the rating elements on the current page and append them to the rating list\n",
    "    rating_tag = driver.find_elements(By.XPATH, '//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "    ratings_page = [i.text for i in rating_tag]\n",
    "    \n",
    "    # Find all the review summary elements on the current page and append them to the review_summary list\n",
    "    review_tag = driver.find_elements(By.XPATH, '//p[@class=\"_2-N8zT\"]')\n",
    "    review_summary_page = [i.text for i in review_tag]\n",
    "    \n",
    "    # Find all the full review elements on the current page and append them to the full_review list\n",
    "    full_tag = driver.find_elements(By.XPATH, '//div[@class=\"t-ZTKy\"]')\n",
    "    full_review_page = [i.text for i in full_tag]\n",
    "    \n",
    "    # Use the zip_longest function to ensure all lists are of the same length\n",
    "    zipped_data = itertools.zip_longest(ratings_page, review_summary_page, full_review_page, fillvalue='NA')\n",
    "    \n",
    "    # Unzip the zipped data into separate lists\n",
    "    ratings_page, review_summary_page, full_review_page = zip(*zipped_data)\n",
    "    \n",
    "    # Append the page data to the main lists\n",
    "    ratings.extend(ratings_page)\n",
    "    review_summary.extend(review_summary_page)\n",
    "    full_review.extend(full_review_page)\n",
    "        \n",
    "    # Click on the next button to move to the next page\n",
    "    nexts = driver.find_element(By.CLASS_NAME, '_1LKTO3')\n",
    "    nexts.click()\n",
    "    \n",
    "    # Use a time.sleep() to wait for the next page to load\n",
    "    time.sleep(10)\n",
    "    \n",
    "# Slice the lists to ensure they are all the same length\n",
    "ratings = ratings[:100]\n",
    "review_summarys = review_summary[:100]\n",
    "full_reviews = full_review[:100]\n",
    "\n",
    "# Create a pandas dataframe from the scraped data and display it\n",
    "df_iphone= pd.DataFrame({'Rating': ratings,\n",
    "                         'Review Summary': review_summarys,\n",
    "                         'Full Review': full_reviews})\n",
    "df_iphone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fae07",
   "metadata": {},
   "source": [
    "### Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for ‚Äúsneakers‚Äù in the search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cc623c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "627f7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the flipkart.com on automated chrome browser\n",
    "driver.get('https://www.flipkart.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9715d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the product as required in the question\n",
    "sneakers = driver.find_element(By.CLASS_NAME,\"_3704LK\")\n",
    "sneakers.send_keys(\"sneakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb10ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on search button to search the required data \n",
    "search = driver.find_element(By.CLASS_NAME,'L0Z3Pu')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4ae5e6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list to store the data\n",
    "brand =[]\n",
    "product_description=[]\n",
    "price\n",
    "\n",
    "# Set the start and end pages to scrape\n",
    "start = 0\n",
    "end = 3\n",
    "# Loop through the pages and scrape data\n",
    "for page in range(start, end):\n",
    "    \n",
    "    # Find all the brand elements on the current page and append them to the brand list\n",
    "    brand_tag = driver.find_elements(By.XPATH,'//div[@class=\"_2WkVRV\"]')\n",
    "    for i in brand_tag:\n",
    "        brand.append(i.text)\n",
    "        \n",
    "        \n",
    "    # Find all the product description elements on the current page and append them to the product description list\n",
    "    product_tag = driver.find_elements(By.XPATH, '//a[@class=\"IRpwTa\"]')\n",
    "    for i in product_tag:\n",
    "        product_description.append(i.text)\n",
    "        \n",
    "    # Find all the price elements on the current page and append them to the price list\n",
    "    price_tag = driver.find_elements(By.XPATH, '//a[@class=\"IRpwTa\"]')\n",
    "    for i in price_tag:\n",
    "        price.append(i.text)\n",
    "        \n",
    "        \n",
    "     # Click on the next button to move to the next page\n",
    "    nexts = driver.find_element(By.CLASS_NAME, '_1LKTO3')\n",
    "    nexts.click()\n",
    "    \n",
    "    # Use a time.sleep() to wait for the next page to load\n",
    "    time.sleep(3)\n",
    "    \n",
    "# Truncate the lists to 100 elements if necessary    \n",
    "Brand = brand[:100]\n",
    "Product_description =product_description[:100]\n",
    "Price = price[:100]\n",
    "\n",
    "\n",
    "# Create a pandas dataframe from the scraped data and display it\n",
    "df_sneakers= pd.DataFrame({'Brand': Brand,\n",
    "                         'Product Description': Product_description,\n",
    "                         'Price': Price})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee7da1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Layasa</td>\n",
       "      <td>Stylish Casual Sports Shoe Sneakers Sneakers F...</td>\n",
       "      <td>‚Çπ232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RapidBox</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EZDEZARIO</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Lightweight,Comfort,Summer,Trendy,Walking,Outd...</td>\n",
       "      <td>‚Çπ149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Labbin</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>‚Çπ219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>WHITE WALKERS</td>\n",
       "      <td>Stylish &amp; Trending Outdoor Walking Comfortable...</td>\n",
       "      <td>‚Çπ664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>BIG FOX</td>\n",
       "      <td>Advantage Genz Sneakers For Men Sneakers For Men</td>\n",
       "      <td>‚Çπ179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Footox</td>\n",
       "      <td>Sneaker for Mens | Casual Shoes | Men Shoes Sn...</td>\n",
       "      <td>‚Çπ224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>aadi</td>\n",
       "      <td>Synthetic Leather |Lightweight|Comfort|Summer|...</td>\n",
       "      <td>‚Çπ179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>Latest Exclusive Affordable Collection of Tren...</td>\n",
       "      <td>‚Çπ335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Brand                                Product Description  \\\n",
       "0                Layasa  Stylish Casual Sports Shoe Sneakers Sneakers F...   \n",
       "1              RapidBox                                   Sneakers For Men   \n",
       "2             EZDEZARIO                                   Sneakers For Men   \n",
       "3                  aadi  Lightweight,Comfort,Summer,Trendy,Walking,Outd...   \n",
       "4                Labbin                                   Sneakers For Men   \n",
       "..                  ...                                                ...   \n",
       "95        WHITE WALKERS  Stylish & Trending Outdoor Walking Comfortable...   \n",
       "96              BIG FOX   Advantage Genz Sneakers For Men Sneakers For Men   \n",
       "97               Footox  Sneaker for Mens | Casual Shoes | Men Shoes Sn...   \n",
       "98                 aadi  Synthetic Leather |Lightweight|Comfort|Summer|...   \n",
       "99  World Wear Footwear  Latest Exclusive Affordable Collection of Tren...   \n",
       "\n",
       "   Price  \n",
       "0   ‚Çπ232  \n",
       "1   ‚Çπ207  \n",
       "2   ‚Çπ149  \n",
       "3   ‚Çπ149  \n",
       "4   ‚Çπ219  \n",
       "..   ...  \n",
       "95  ‚Çπ664  \n",
       "96  ‚Çπ179  \n",
       "97  ‚Çπ224  \n",
       "98  ‚Çπ179  \n",
       "99  ‚Çπ335  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sneakers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279fe67",
   "metadata": {},
   "source": [
    "### Q7: Go to webpage https://www.amazon.in/ Enter ‚ÄúLaptop‚Äù in the search field and then click the search icon. Then set CPU Type filter to ‚ÄúIntel Core i7‚Äù as shown in the below image:\n",
    "\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f09b2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3900a2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the amazon.in on automated chrome browser\n",
    "driver.get('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b38be058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering the product as required in the question\n",
    "laptop = driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "laptop.send_keys(\"Laptop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4603c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on search button to search the required data \n",
    "search = driver.find_element(By.XPATH,'/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div/span/input')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68b909e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter intel core i7 laptop\n",
    "i7= driver.find_element(By.XPATH,'//*[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]/span/a/span')\n",
    "i7.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e1e579d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HP Victus Gaming Latest 12th Gen Intel Core i7...</td>\n",
       "      <td>[4.3, (118)]</td>\n",
       "      <td>79,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...</td>\n",
       "      <td>[4.3, (80)]</td>\n",
       "      <td>1,00,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASUS TUF Gaming F15 (2022), 15.6\"(39.62 cms) F...</td>\n",
       "      <td>[4.4, (70)]</td>\n",
       "      <td>92,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS TUF Dash F15, Intel Core i7-12650H 12th G...</td>\n",
       "      <td>[4.0, (2)]</td>\n",
       "      <td>31,592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Renewed) HP 840g3 Elitebook 14 Inch Screen La...</td>\n",
       "      <td>[3.0, (2)]</td>\n",
       "      <td>84,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...</td>\n",
       "      <td>[4.3, (51)]</td>\n",
       "      <td>31,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Renewed) HP ELITEBOOK 840 G5 (Core i7 8th GEN...</td>\n",
       "      <td>[New, to, Amazon]</td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASUS Vivobook S15 OLED 2022, 15.6\" 39.62 cms F...</td>\n",
       "      <td>[4.0, (10)]</td>\n",
       "      <td>94,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASUS TUF Gaming F15 (2022), 15.6\" (39.62 cms) ...</td>\n",
       "      <td>[4.1, (43)]</td>\n",
       "      <td>84,910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dell Inspiron 5320 Laptop, Intel i7-1260P, 16G...</td>\n",
       "      <td>[3.7, (12)]</td>\n",
       "      <td>30,393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title             Rating  \\\n",
       "0  HP Victus Gaming Latest 12th Gen Intel Core i7...       [4.3, (118)]   \n",
       "1  Lenovo IdeaPad Slim 5 Intel Core i7 12th Gen 1...        [4.3, (80)]   \n",
       "2  ASUS TUF Gaming F15 (2022), 15.6\"(39.62 cms) F...        [4.4, (70)]   \n",
       "3  ASUS TUF Dash F15, Intel Core i7-12650H 12th G...         [4.0, (2)]   \n",
       "4  (Renewed) HP 840g3 Elitebook 14 Inch Screen La...         [3.0, (2)]   \n",
       "5  HP Pavilion 14 12th Gen Intel Core i7 16GB SDR...        [4.3, (51)]   \n",
       "6  (Renewed) HP ELITEBOOK 840 G5 (Core i7 8th GEN...  [New, to, Amazon]   \n",
       "7  ASUS Vivobook S15 OLED 2022, 15.6\" 39.62 cms F...        [4.0, (10)]   \n",
       "8  ASUS TUF Gaming F15 (2022), 15.6\" (39.62 cms) ...        [4.1, (43)]   \n",
       "9  Dell Inspiron 5320 Laptop, Intel i7-1260P, 16G...        [3.7, (12)]   \n",
       "\n",
       "      Price  \n",
       "0    79,990  \n",
       "1  1,00,990  \n",
       "2    92,990  \n",
       "3    31,592  \n",
       "4    84,999  \n",
       "5    31,490  \n",
       "6    84,990  \n",
       "7    94,990  \n",
       "8    84,910  \n",
       "9    30,393  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty lists to store the scraped data\n",
    "title = []\n",
    "rating = []\n",
    "Price = []\n",
    "\n",
    "# Find all the title tags using XPath and append the text of each tag to the `title` list\n",
    "title_tag = driver.find_elements(By.XPATH,'//h2[@class=\"a-size-mini a-spacing-none a-color-base s-line-clamp-2\"]')\n",
    "for i in title_tag:\n",
    "    title.append(i.text)\n",
    "\n",
    "# Find all the rating tags using XPath, split the text of each tag into a list of words, and append the list to the `rating` list\n",
    "rating_tag = driver.find_elements(By.XPATH, '//div[@class=\"a-row a-size-small\"]')\n",
    "for i in rating_tag:\n",
    "    rating.append(i.text.split())\n",
    "\n",
    "# Find all the price tags using XPath and append the text of each tag to the `Price` list\n",
    "Price_tag = driver.find_elements(By.XPATH, '//span[@class=\"a-price-whole\"]')\n",
    "for i in Price_tag:\n",
    "    Price.append(i.text)\n",
    "\n",
    "# Select only the first 10 items in each list to limit the data to the top 10 items\n",
    "Title = title[:10]\n",
    "rating = rating[:10]\n",
    "Price = Price[:10]\n",
    "\n",
    "# Create a pandas dataframe from the scraped data and display it\n",
    "df_laptop = pd.DataFrame({'Title': Title,\n",
    "                          'Rating': rating,\n",
    "                          'Price': Price})\n",
    "df_laptop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92262d24",
   "metadata": {},
   "source": [
    "### Q8: Write a python program to scrape data for Top 1000 Quotes of All Time.3. Than scrap a) Quote b) Author c) Type Of QuotesThe above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuotes\n",
    "3. Than scrap a) Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7586ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4822823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the azquotes.com on automated chrome browser\n",
    "driver.get('https://www.azquotes.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6968594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on top quotes button to search the required data \n",
    "quotes = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]')\n",
    "quotes.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fb5231ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quotes</th>\n",
       "      <th>Author</th>\n",
       "      <th>Type Of Quotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The essence of strategy is choosing what not t...</td>\n",
       "      <td>Michael Porter</td>\n",
       "      <td>Essence, Deep Thought, Transcendentalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One cannot and must not try to erase the past ...</td>\n",
       "      <td>Golda Meir</td>\n",
       "      <td>Inspiration, Past, Trying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patriotism means to stand by the country. It d...</td>\n",
       "      <td>Theodore Roosevelt</td>\n",
       "      <td>Country, Peace, War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Death is something inevitable. When a man has ...</td>\n",
       "      <td>Nelson Mandela</td>\n",
       "      <td>Inspirational, Motivational, Death</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You have to love a nation that celebrates its ...</td>\n",
       "      <td>Erma Bombeck</td>\n",
       "      <td>4th Of July, Food, Patriotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>Regret for the things we did can be tempered b...</td>\n",
       "      <td>Sydney J. Harris</td>\n",
       "      <td>Love, Inspirational, Motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>America... just a nation of two hundred millio...</td>\n",
       "      <td>Hunter S. Thompson</td>\n",
       "      <td>Gun, Two, Qualms About</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>For every disciplined effort there is a multip...</td>\n",
       "      <td>Jim Rohn</td>\n",
       "      <td>Inspirational, Greatness, Best Effort</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>The spiritual journey is individual, highly pe...</td>\n",
       "      <td>Ram Dass</td>\n",
       "      <td>Spiritual, Truth, Yoga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>The mind is not a vessel to be filled but a fi...</td>\n",
       "      <td>Plutarch</td>\n",
       "      <td>Inspirational, Leadership, Education</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Quotes              Author  \\\n",
       "0    The essence of strategy is choosing what not t...      Michael Porter   \n",
       "1    One cannot and must not try to erase the past ...          Golda Meir   \n",
       "2    Patriotism means to stand by the country. It d...  Theodore Roosevelt   \n",
       "3    Death is something inevitable. When a man has ...      Nelson Mandela   \n",
       "4    You have to love a nation that celebrates its ...        Erma Bombeck   \n",
       "..                                                 ...                 ...   \n",
       "995  Regret for the things we did can be tempered b...    Sydney J. Harris   \n",
       "996  America... just a nation of two hundred millio...  Hunter S. Thompson   \n",
       "997  For every disciplined effort there is a multip...            Jim Rohn   \n",
       "998  The spiritual journey is individual, highly pe...            Ram Dass   \n",
       "999  The mind is not a vessel to be filled but a fi...            Plutarch   \n",
       "\n",
       "                               Type Of Quotes  \n",
       "0    Essence, Deep Thought, Transcendentalism  \n",
       "1                   Inspiration, Past, Trying  \n",
       "2                         Country, Peace, War  \n",
       "3          Inspirational, Motivational, Death  \n",
       "4                4th Of July, Food, Patriotic  \n",
       "..                                        ...  \n",
       "995         Love, Inspirational, Motivational  \n",
       "996                    Gun, Two, Qualms About  \n",
       "997     Inspirational, Greatness, Best Effort  \n",
       "998                    Spiritual, Truth, Yoga  \n",
       "999      Inspirational, Leadership, Education  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create empty list to store the data\n",
    "Quote =[]\n",
    "Author =[]\n",
    "Type_of_quotes=[]\n",
    "# Set the start and end pages to scrape\n",
    "start = 0\n",
    "end = 10\n",
    "# Loop through the pages and scrape data\n",
    "for page in range(start, end):\n",
    "\n",
    "    quote_tag = driver.find_elements(By.XPATH,'//a[@class=\"title\"]')\n",
    "    for i in quote_tag:\n",
    "        Quote.append(i.text)\n",
    "        \n",
    "    author_tag = driver.find_elements(By.XPATH,'//div[@class=\"author\"]')\n",
    "    for i in author_tag:\n",
    "        Author.append(i.text)\n",
    "    \n",
    "    type_of_quotes_tag = driver.find_elements(By.XPATH,'//div[@class=\"tags\"]')\n",
    "    for i in type_of_quotes_tag:\n",
    "        Type_of_quotes.append(i.text)\n",
    "    \n",
    "    # Click on the next button to move to the next page\n",
    "    nexts = driver.find_element(By.XPATH, '/html/body/div[1]/div[2]/div/div/div/div[1]/div/div[4]/li[12]')\n",
    "    nexts.click()\n",
    "    \n",
    "    # Use a time.sleep() to wait for the next page to load\n",
    "    time.sleep(3)\n",
    "df_quotes = pd.DataFrame({'Quotes':Quote,'Author':Author,'Type Of Quotes':Type_of_quotes})\n",
    "\n",
    "df_quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e5bbb1",
   "metadata": {},
   "source": [
    "### Q9: Write a python program to display list of respected former Prime Ministers of India(i.e. Name, Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/.This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.jagranjosh.com/\n",
    "2. Then You have to click on the GK option\n",
    "3. Then click on the List of all Prime Ministers of India\n",
    "4. Then scrap the mentioned data and make theDataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "01cfac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d802004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the jagranjosh.com on automated chrome browser\n",
    "driver.get('https://www.jagranjosh.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b8f2c32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on GK button to search the required data \n",
    "gk = driver.find_element(By.XPATH,'/html/body/div/div[1]/div/div[1]/div/div[5]/div/div[1]/header/div[3]/ul/li[9]/a')\n",
    "gk.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "36e03378",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clicking on pm section to search the required data\n",
    "pm = driver.find_element(By.XPATH,'/html/body/div[1]/div/div/div[2]/div/div[29]/div/div/ul/li/ul[2]/li[11]/a')\n",
    "pm.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9fccd86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Born Dead</th>\n",
       "      <th>Term Of Office</th>\n",
       "      <th>Remarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jawahar Lal Nehru</td>\n",
       "      <td>(1889‚Äì1964)</td>\n",
       "      <td>15 August 1947 to 27 May 1964\\n16 years, 286 days</td>\n",
       "      <td>The first prime minister of India and the long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gulzarilal Nanda (Acting)</td>\n",
       "      <td>(1898-1998)</td>\n",
       "      <td>27 May 1964 to 9 June 1964,\\n13 days</td>\n",
       "      <td>First acting PM of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lal Bahadur Shastri</td>\n",
       "      <td>(1904‚Äì1966)</td>\n",
       "      <td>9 June 1964 to 11 January 1966\\n1 year, 216 days</td>\n",
       "      <td>He has given the slogan of 'Jai Jawan Jai Kisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gulzari Lal Nanda  (Acting)</td>\n",
       "      <td>(1898-1998)</td>\n",
       "      <td>11 January 1966 to 24 January 1966\\n13 days</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indira Gandhi</td>\n",
       "      <td>(1917‚Äì1984)</td>\n",
       "      <td>24 January 1966 to 24 March 1977\\n11 years, 59...</td>\n",
       "      <td>First female Prime Minister of India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Morarji Desai</td>\n",
       "      <td>(1896‚Äì1995)</td>\n",
       "      <td>24 March 1977 to  28 July 1979 \\n2 year, 126 days</td>\n",
       "      <td>Oldest to become PM (81 years old) and first t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Charan Singh</td>\n",
       "      <td>(1902‚Äì1987)</td>\n",
       "      <td>28 July 1979 to 14 January 1980\\n170 days</td>\n",
       "      <td>Only PM who did not face the Parliament</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Indira Gandhi</td>\n",
       "      <td>(1917‚Äì1984)</td>\n",
       "      <td>14 January 1980 to 31 October 1984\\n4 years, 2...</td>\n",
       "      <td>The first lady who served as PM for the second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rajiv Gandhi</td>\n",
       "      <td>(1944‚Äì1991)</td>\n",
       "      <td>31 October 1984 to 2 December 1989\\n5 years, 3...</td>\n",
       "      <td>Youngest to become PM (40 years old)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>V. P. Singh</td>\n",
       "      <td>(1931‚Äì2008)</td>\n",
       "      <td>2 December 1989 to 10 November 1990\\n343 days</td>\n",
       "      <td>First PM to step down after a vote of no confi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chandra Shekhar</td>\n",
       "      <td>(1927‚Äì2007)</td>\n",
       "      <td>10 November 1990 to 21 June 1991\\n223 days</td>\n",
       "      <td>He belongs to  Samajwadi Janata Party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>P. V. Narasimha Rao</td>\n",
       "      <td>(1921‚Äì2004)</td>\n",
       "      <td>21 June 1991 to 16 May 1996\\n4 years, 330 days</td>\n",
       "      <td>First PM from south India</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Atal Bihari Vajpayee</td>\n",
       "      <td>(1924- 2018)</td>\n",
       "      <td>16 May 1996 to 1 June 1996\\n16 days</td>\n",
       "      <td>PM for shortest tenure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>H. D. Deve Gowda</td>\n",
       "      <td>(born 1933)</td>\n",
       "      <td>1 June 1996 to 21 April 1997\\n324 days</td>\n",
       "      <td>He belongs to  Janata Dal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Inder Kumar Gujral</td>\n",
       "      <td>(1919‚Äì2012)</td>\n",
       "      <td>21 April 1997 to 19 March 1998 \\n332 days</td>\n",
       "      <td>------</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Atal Bihari Vajpayee</td>\n",
       "      <td>(1924-2018)</td>\n",
       "      <td>19 March 1998 to 22 May 2004 \\n6 years, 64 days</td>\n",
       "      <td>The first non-congress PM who completed a ful...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Manmohan Singh</td>\n",
       "      <td>(born 1932)</td>\n",
       "      <td>22 May 2004 to 26 May 2014   \\n10 years, 4 days</td>\n",
       "      <td>First Sikh PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Narendra Modi</td>\n",
       "      <td>(born 1950)</td>\n",
       "      <td>26 May 2014 - Present</td>\n",
       "      <td>4th Prime Minister of India who served two con...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name     Born Dead  \\\n",
       "0             Jawahar Lal Nehru   (1889‚Äì1964)   \n",
       "1     Gulzarilal Nanda (Acting)   (1898-1998)   \n",
       "2           Lal Bahadur Shastri   (1904‚Äì1966)   \n",
       "3   Gulzari Lal Nanda  (Acting)   (1898-1998)   \n",
       "4                 Indira Gandhi   (1917‚Äì1984)   \n",
       "5                 Morarji Desai   (1896‚Äì1995)   \n",
       "6                  Charan Singh   (1902‚Äì1987)   \n",
       "7                 Indira Gandhi   (1917‚Äì1984)   \n",
       "8                  Rajiv Gandhi   (1944‚Äì1991)   \n",
       "9                   V. P. Singh   (1931‚Äì2008)   \n",
       "10              Chandra Shekhar   (1927‚Äì2007)   \n",
       "11          P. V. Narasimha Rao   (1921‚Äì2004)   \n",
       "12         Atal Bihari Vajpayee  (1924- 2018)   \n",
       "13             H. D. Deve Gowda   (born 1933)   \n",
       "14           Inder Kumar Gujral   (1919‚Äì2012)   \n",
       "15         Atal Bihari Vajpayee   (1924-2018)   \n",
       "16               Manmohan Singh   (born 1932)   \n",
       "17                Narendra Modi   (born 1950)   \n",
       "\n",
       "                                       Term Of Office  \\\n",
       "0   15 August 1947 to 27 May 1964\\n16 years, 286 days   \n",
       "1                27 May 1964 to 9 June 1964,\\n13 days   \n",
       "2    9 June 1964 to 11 January 1966\\n1 year, 216 days   \n",
       "3         11 January 1966 to 24 January 1966\\n13 days   \n",
       "4   24 January 1966 to 24 March 1977\\n11 years, 59...   \n",
       "5   24 March 1977 to  28 July 1979 \\n2 year, 126 days   \n",
       "6           28 July 1979 to 14 January 1980\\n170 days   \n",
       "7   14 January 1980 to 31 October 1984\\n4 years, 2...   \n",
       "8   31 October 1984 to 2 December 1989\\n5 years, 3...   \n",
       "9       2 December 1989 to 10 November 1990\\n343 days   \n",
       "10         10 November 1990 to 21 June 1991\\n223 days   \n",
       "11     21 June 1991 to 16 May 1996\\n4 years, 330 days   \n",
       "12                16 May 1996 to 1 June 1996\\n16 days   \n",
       "13             1 June 1996 to 21 April 1997\\n324 days   \n",
       "14          21 April 1997 to 19 March 1998 \\n332 days   \n",
       "15    19 March 1998 to 22 May 2004 \\n6 years, 64 days   \n",
       "16    22 May 2004 to 26 May 2014   \\n10 years, 4 days   \n",
       "17                              26 May 2014 - Present   \n",
       "\n",
       "                                              Remarks  \n",
       "0   The first prime minister of India and the long...  \n",
       "1                            First acting PM of India  \n",
       "2   He has given the slogan of 'Jai Jawan Jai Kisa...  \n",
       "3                                                   -  \n",
       "4                First female Prime Minister of India  \n",
       "5   Oldest to become PM (81 years old) and first t...  \n",
       "6             Only PM who did not face the Parliament  \n",
       "7   The first lady who served as PM for the second...  \n",
       "8                Youngest to become PM (40 years old)  \n",
       "9   First PM to step down after a vote of no confi...  \n",
       "10              He belongs to  Samajwadi Janata Party  \n",
       "11                          First PM from south India  \n",
       "12                             PM for shortest tenure  \n",
       "13                          He belongs to  Janata Dal  \n",
       "14                                             ------  \n",
       "15   The first non-congress PM who completed a ful...  \n",
       "16                                      First Sikh PM  \n",
       "17  4th Prime Minister of India who served two con...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty lists to store the data\n",
    "Name=[]\n",
    "Born_Dead=[]\n",
    "Term_of_office=[]\n",
    "Remarks=[]\n",
    "\n",
    "# Find all td elements on the page and append their text to the Name list\n",
    "name_tag = driver.find_elements(By.TAG_NAME,'td')\n",
    "for i in name_tag:\n",
    "    Name.append(i.text)\n",
    "\n",
    "# Find all td elements on the page and append their text to the Born_Dead list\n",
    "brand_tag = driver.find_elements(By.TAG_NAME,'td')\n",
    "for i in brand_tag:\n",
    "    Born_Dead.append(i.text)\n",
    "\n",
    "# Find all td elements on the page and append their text to the Term_of_office list\n",
    "term_tag = driver.find_elements(By.TAG_NAME,'td')\n",
    "for i in term_tag:\n",
    "    Term_of_office.append(i.text)\n",
    "\n",
    "# Find all td elements on the page and append their text to the Remarks list\n",
    "Remarks_tag = driver.find_elements(By.TAG_NAME,'td')\n",
    "for i in Remarks_tag:\n",
    "    Remarks.append(i.text)\n",
    "\n",
    "# Slice the Name list to extract every 5th element starting from the second element\n",
    "# This extracts the values for the \"Name\" column of the table\n",
    "Name = Name[1:90:5]\n",
    "\n",
    "# Slice the Born_Dead list to extract every 5th element starting from the third element\n",
    "# This extracts the values for the \"Born Dead\" column of the table\n",
    "Born_Dead = Born_Dead[2:90:5]\n",
    "\n",
    "# Slice the Term_of_office list to extract every 5th element starting from the fourth element\n",
    "# This extracts the values for the \"Term Of Office\" column of the table\n",
    "Term_of_office = Term_of_office[3:90:5]\n",
    "\n",
    "# Slice the Remarks list to extract every 5th element starting from the fifth element\n",
    "# This extracts the values for the \"Remarks\" column of the table\n",
    "Remarks = Remarks[4:90:5]\n",
    "\n",
    "# Create a pandas DataFrame using a dictionary of the lists\n",
    "df_prime = pd.DataFrame({'Name':Name,'Born Dead':Born_Dead,'Term Of Office':Term_of_office,'Remarks':Remarks})\n",
    "\n",
    "# Print the DataFrame to the console\n",
    "df_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1424ed",
   "metadata": {},
   "source": [
    "### Q10: Write a python program to display list of 50 Most expensive cars in the world (i.e. Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpagehttps://www.motor1.com/\n",
    "2. Then You have to click on the List option from Dropdown menu on leftside.\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap the mentioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b47904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let first connect to the driver \n",
    "driver = webdriver.Chrome(r\"C:\\Users\\ASUS\\Downloads\\chromedriver_win32.zip\\chormedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a5bbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the motor1.com on automated chrome browser\n",
    "driver.get('https://www.motor1.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15563322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the text in the search box\n",
    "data = driver.find_element(By.XPATH,'/html/body/div[3]/div[2]/div/div/div[3]/div/div/div/form/input')\n",
    "data.send_keys('50 Most expensive cars in the world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6298838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the search button\n",
    "searchs= driver.find_element(By.XPATH,'/html/body/div[3]/div[2]/div/div/div[3]/div/div/div/form/button[1]')\n",
    "searchs.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "890f1b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# click on the 50 most expensive cars section\n",
    "cars = driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div/div[1]/div/div/div[1]/div/div[1]')\n",
    "cars.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7fa0ced3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Car Name</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>De Tomaso P72</td>\n",
       "      <td>Price: $1.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ferrari LaFerrari</td>\n",
       "      <td>Price: $1.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pagani Huayra</td>\n",
       "      <td>Price: $1.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Czinger 21C</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ferrari Monza</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gordon Murray T.33</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Koenigsegg Gemera</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Zenvo TSR-S</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hennessey Venom F5</td>\n",
       "      <td>Price: $1.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bentley Bacalar</td>\n",
       "      <td>Price: $1.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hispano Suiza Carmen Boulogne</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Bentley Mulliner Batur</td>\n",
       "      <td>Price: $1.9 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Deus Vayanne</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SSC Tuatara</td>\n",
       "      <td>Price: $2.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lotus Evija</td>\n",
       "      <td>Price: $2.0 Million*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Aston Martin Vulcan</td>\n",
       "      <td>Price: $2.1 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Delage D12</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>McLaren Speedtail</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Rimac Nevera</td>\n",
       "      <td>Price: $2.3 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Pagani Utopia</td>\n",
       "      <td>Price: $2.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Pininfarina Battista</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Ferrari FXX K Evo</td>\n",
       "      <td>Price: $2.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Gordon Murray T.50</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Lamborghini Countach</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Mercedes-AMG Project One</td>\n",
       "      <td>Price: $2.6 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Aston Martin Victor</td>\n",
       "      <td>Price: $2.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hennessey Venom F5 Roadster</td>\n",
       "      <td>Price: $3.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Koenigsegg Jesko</td>\n",
       "      <td>$3.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Aston Martin Valkyrie</td>\n",
       "      <td>Price: $3.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>W Motors Lykan Hypersport</td>\n",
       "      <td>Price: $3.2 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>McLaren Solus</td>\n",
       "      <td>Price: $3.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Pagani Huayra Roadster BC</td>\n",
       "      <td>$3.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Bugatti Chiron Pur Sport</td>\n",
       "      <td>Price: $3.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Lamborghini Sian</td>\n",
       "      <td>Price: $3.6 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Koenigsegg CC850</td>\n",
       "      <td>Price: $3.6 million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Bugatti Chiron Super Sport 300+</td>\n",
       "      <td>Price: $3.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Lamborghini Veneno</td>\n",
       "      <td>Price: $3.9 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Bugatti Bolide</td>\n",
       "      <td>Price: $4.5 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Bugatti Mistral</td>\n",
       "      <td>Price: $4.7 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Pagani Huayra Imola</td>\n",
       "      <td>Price: $5.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Bugatti Divo</td>\n",
       "      <td>Price: $5.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>SP Automotive Chaos</td>\n",
       "      <td>Price: $5.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Pagani Codalunga</td>\n",
       "      <td>Price: $6.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Mercedes-Maybach Exelero</td>\n",
       "      <td>Price: $7.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Bugatti Centodieci</td>\n",
       "      <td>Price: $8.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Bugatti Chiron Profil√©e</td>\n",
       "      <td>Price: $9.0 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Rolls-Royce Sweptail</td>\n",
       "      <td>Price: $10.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Bugatti La Voiture Noire</td>\n",
       "      <td>Price: $12.8 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Rolls-Royce Boat Tail*</td>\n",
       "      <td>Price: $13.4 Million</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Most Expensive Cars In The World</td>\n",
       "      <td>Price: $28.0 Million (est.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Car Name                        Price\n",
       "0                      De Tomaso P72          Price: $1.3 Million\n",
       "1                  Ferrari LaFerrari          Price: $1.4 Million\n",
       "2                      Pagani Huayra          Price: $1.4 Million\n",
       "3                        Czinger 21C          Price: $1.7 Million\n",
       "4                      Ferrari Monza          Price: $1.7 Million\n",
       "5                 Gordon Murray T.33          Price: $1.7 Million\n",
       "6                  Koenigsegg Gemera          Price: $1.7 Million\n",
       "7                        Zenvo TSR-S          Price: $1.7 Million\n",
       "8                 Hennessey Venom F5          Price: $1.7 Million\n",
       "9                    Bentley Bacalar          Price: $1.8 Million\n",
       "10     Hispano Suiza Carmen Boulogne          Price: $1.9 Million\n",
       "11            Bentley Mulliner Batur          Price: $1.9 Million\n",
       "12                      Deus Vayanne          Price: $2.0 Million\n",
       "13                       SSC Tuatara          Price: $2.0 Million\n",
       "14                       Lotus Evija         Price: $2.0 Million*\n",
       "15               Aston Martin Vulcan          Price: $2.1 Million\n",
       "16                        Delage D12          Price: $2.3 Million\n",
       "17                 McLaren Speedtail          Price: $2.3 Million\n",
       "18                      Rimac Nevera          Price: $2.3 Million\n",
       "19                     Pagani Utopia          Price: $2.4 Million\n",
       "20              Pininfarina Battista          Price: $2.5 Million\n",
       "21                 Ferrari FXX K Evo          Price: $2.5 Million\n",
       "22                Gordon Murray T.50          Price: $2.6 Million\n",
       "23              Lamborghini Countach          Price: $2.6 Million\n",
       "24          Mercedes-AMG Project One          Price: $2.6 Million\n",
       "25               Aston Martin Victor          Price: $2.7 Million\n",
       "26       Hennessey Venom F5 Roadster          Price: $3.0 Million\n",
       "27                  Koenigsegg Jesko                 $3.0 Million\n",
       "28             Aston Martin Valkyrie          Price: $3.0 Million\n",
       "29         W Motors Lykan Hypersport          Price: $3.2 Million\n",
       "30                     McLaren Solus          Price: $3.4 Million\n",
       "31         Pagani Huayra Roadster BC                 $3.5 Million\n",
       "32          Bugatti Chiron Pur Sport          Price: $3.5 Million\n",
       "33                  Lamborghini Sian          Price: $3.6 Million\n",
       "34                  Koenigsegg CC850          Price: $3.6 million\n",
       "35   Bugatti Chiron Super Sport 300+          Price: $3.7 Million\n",
       "36                Lamborghini Veneno          Price: $3.9 Million\n",
       "37                    Bugatti Bolide          Price: $4.5 Million\n",
       "38                   Bugatti Mistral          Price: $4.7 Million\n",
       "39               Pagani Huayra Imola          Price: $5.0 Million\n",
       "40                      Bugatti Divo          Price: $5.4 Million\n",
       "41               SP Automotive Chaos          Price: $5.8 Million\n",
       "42                  Pagani Codalunga          Price: $6.4 Million\n",
       "43          Mercedes-Maybach Exelero          Price: $7.4 Million\n",
       "44                Bugatti Centodieci          Price: $8.0 Million\n",
       "45           Bugatti Chiron Profil√©e          Price: $9.0 Million\n",
       "46              Rolls-Royce Sweptail         Price: $10.8 Million\n",
       "47          Bugatti La Voiture Noire         Price: $12.8 Million\n",
       "48            Rolls-Royce Boat Tail*         Price: $13.4 Million\n",
       "49  Most Expensive Cars In The World  Price: $28.0 Million (est.)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create empty lists to store the scraped data\n",
    "cars_name = []\n",
    "cars_price = []\n",
    "\n",
    "# Find all the car tags using XPath and append the text of each tag to the `cars_name` list\n",
    "cars_tags = driver.find_elements(By.XPATH,'//h3[@class=\"subheader\"]')\n",
    "for i in cars_tags:\n",
    "    cars_name.append(i.text)\n",
    "\n",
    "# Find all the price tags using the `TAG_NAME` locator for `strong` tags and append the text of each tag to the `cars_price` list\n",
    "price_tags = driver.find_elements(By.TAG_NAME,'strong')\n",
    "for i in price_tags:\n",
    "    cars_price.append(i.text)\n",
    "\n",
    "# Create a pandas dataframe from the scraped data and display it\n",
    "df_cars = pd.DataFrame({'Car Name':cars_name,'Price':cars_price})\n",
    "\n",
    "# dropping the rows having NaN values\n",
    "df = df_cars.drop(3)\n",
    " \n",
    "# To reset the indices\n",
    "df = df.reset_index(drop=True)\n",
    " \n",
    "# Print the dataframe\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e80e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397df8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
